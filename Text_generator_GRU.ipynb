{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "IWnkEno6opzB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
      ],
      "metadata": {
        "id": "OKGHdF0BpXeh"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Read, then decode for py2 compat.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "# length of text is the number of characters in it\n",
        "print(f'Length of text: {len(text)} characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qsom9C41pl-K",
        "outputId": "8ca62eff-6065-4633-b3cd-f928bbebf8fe"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of text: 1115394 characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Take a look at the first 250 characters in text\n",
        "print(text[:250])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "COJSz2Hlptv0",
        "outputId": "f1885f1b-b1f9-4271-88c1-d2ba6a1d5b8d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First Citizen:\n",
            "Before we proceed any further, hear me speak.\n",
            "\n",
            "All:\n",
            "Speak, speak.\n",
            "\n",
            "First Citizen:\n",
            "You are all resolved rather to die than to famish?\n",
            "\n",
            "All:\n",
            "Resolved. resolved.\n",
            "\n",
            "First Citizen:\n",
            "First, you know Caius Marcius is chief enemy to the people.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(f'{len(vocab)} unique characters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLrg70k_rrbQ",
        "outputId": "ddd9827c-64a5-4c48-df10-b2062ff4c4e7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "65 unique characters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "example_texts = ['abcdefg', 'xyz']\n",
        "\n",
        "chars = tf.strings.unicode_split(example_texts, input_encoding='UTF-8')\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhOHXSzgz3Sy",
        "outputId": "5ce02037-481d-4957-d5c3-c698b9c0915a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_from_chars = tf.keras.layers.StringLookup(\n",
        "    vocabulary=list(vocab), mask_token=None)"
      ],
      "metadata": {
        "id": "7T85tY3Vz9Ud"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ids = ids_from_chars(chars)\n",
        "ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aQbJM5w-z-tg",
        "outputId": "3ec377d9-a333-449b-c78a-2be191149f19"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[40, 41, 42, 43, 44, 45, 46], [63, 64, 65]]>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chars_from_ids = tf.keras.layers.StringLookup(\n",
        "    vocabulary=ids_from_chars.get_vocabulary(), invert=True, mask_token=None)"
      ],
      "metadata": {
        "id": "CFDS8z7k0F9W"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = chars_from_ids(ids)\n",
        "chars"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WVt2idSq0Xrg",
        "outputId": "353c58fd-fa04-45e0-84f5-e494c19139a6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.RaggedTensor [[b'a', b'b', b'c', b'd', b'e', b'f', b'g'], [b'x', b'y', b'z']]>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tf.strings.reduce_join(chars, axis=-1).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j9Gp0PKA0iqY",
        "outputId": "abf75020-f3ea-4ccc-c04c-3c2a5ae50396"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([b'abcdefg', b'xyz'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def text_from_ids(ids):\n",
        "  return tf.strings.reduce_join(chars_from_ids(ids), axis=-1)"
      ],
      "metadata": {
        "id": "bKjEufcY0nSf"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prediction task"
      ],
      "metadata": {
        "id": "OlzLrmYJ5v5_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_ids = ids_from_chars(tf.strings.unicode_split(text, 'UTF-8'))\n",
        "all_ids"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vX51yYN050J5",
        "outputId": "53ac32c5-c787-4551-ec59-23f3ae747813"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(1115394,), dtype=int64, numpy=array([19, 48, 57, ..., 46,  9,  1])>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ids_dataset = tf.data.Dataset.from_tensor_slices(all_ids)"
      ],
      "metadata": {
        "id": "JzsHASUR54Yd"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for ids in ids_dataset.take(10):\n",
        "    print(chars_from_ids(ids).numpy().decode('utf-8'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AfEDwin957Lo",
        "outputId": "71f79219-36b0-4d98-9464-075c84964400"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "F\n",
            "i\n",
            "r\n",
            "s\n",
            "t\n",
            " \n",
            "C\n",
            "i\n",
            "t\n",
            "i\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "seq_length = 100"
      ],
      "metadata": {
        "id": "ToCzDPtd6B41"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = ids_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for seq in sequences.take(1):\n",
        "  print(chars_from_ids(seq))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cfvBqcEZ6TSH",
        "outputId": "c52605af-1dcf-4cf1-d74d-c8cde94e2afc"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'F' b'i' b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':'\n",
            " b'\\n' b'B' b'e' b'f' b'o' b'r' b'e' b' ' b'w' b'e' b' ' b'p' b'r' b'o'\n",
            " b'c' b'e' b'e' b'd' b' ' b'a' b'n' b'y' b' ' b'f' b'u' b'r' b't' b'h'\n",
            " b'e' b'r' b',' b' ' b'h' b'e' b'a' b'r' b' ' b'm' b'e' b' ' b's' b'p'\n",
            " b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'A' b'l' b'l' b':' b'\\n' b'S' b'p' b'e'\n",
            " b'a' b'k' b',' b' ' b's' b'p' b'e' b'a' b'k' b'.' b'\\n' b'\\n' b'F' b'i'\n",
            " b'r' b's' b't' b' ' b'C' b'i' b't' b'i' b'z' b'e' b'n' b':' b'\\n' b'Y'\n",
            " b'o' b'u' b' '], shape=(101,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#join keywords back to strings\n",
        "for seq in sequences.take(5):\n",
        "  print(text_from_ids(seq).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WtIN4eSA6fM7",
        "outputId": "ada8bc1c-a123-4814-81d4-319cd778a6a0"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n",
            "b'are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you k'\n",
            "b\"now Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us ki\"\n",
            "b\"ll him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be d\"\n",
            "b'one: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citi'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def split_input_target(sequence):\n",
        "    input_text = sequence[:-1]\n",
        "    target_text = sequence[1:]\n",
        "    return input_text, target_text"
      ],
      "metadata": {
        "id": "fRQCa8uj7PkW"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "split_input_target(list(\"Tensorflow\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oNCd68b27Th-",
        "outputId": "b45b9a0d-0ac4-4b7c-f870-0deb6da98387"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['T', 'e', 'n', 's', 'o', 'r', 'f', 'l', 'o'],\n",
              " ['e', 'n', 's', 'o', 'r', 'f', 'l', 'o', 'w'])"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "metadata": {
        "id": "MekMbg9K7WRQ"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "for input_example, target_example in dataset.take(1):\n",
        "    print(\"Input :\", text_from_ids(input_example).numpy())\n",
        "    print(\"Target:\", text_from_ids(target_example).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KUvqg9Z-ABc",
        "outputId": "04864aa7-6cc1-4ab7-8474-9d8fdcbb9cec"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input : b'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou'\n",
            "Target: b'irst Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou '\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create training batches"
      ],
      "metadata": {
        "id": "gGTTHnHM-IKy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = (\n",
        "    dataset\n",
        "    .shuffle(BUFFER_SIZE)\n",
        "    .batch(BATCH_SIZE, drop_remainder=True)\n",
        "    .prefetch(tf.data.experimental.AUTOTUNE))\n",
        "\n",
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "16VPnlrZ-JwG",
        "outputId": "48c408f1-4995-48ad-e66a-884e842a810f"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<_PrefetchDataset element_spec=(TensorSpec(shape=(64, 100), dtype=tf.int64, name=None), TensorSpec(shape=(64, 100), dtype=tf.int64, name=None))>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build The Model"
      ],
      "metadata": {
        "id": "KuqwXN6U-Z9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Length of the vocabulary in StringLookup Layer\n",
        "vocab_size = len(ids_from_chars.get_vocabulary())\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "metadata": {
        "id": "9lkvSJgj-bpL"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class MyModel(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, rnn_units):\n",
        "    super().__init__(self)\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(rnn_units,\n",
        "                                   return_sequences=True,\n",
        "                                   return_state=True)\n",
        "    self.dense = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "  def call(self, inputs, states=None, return_state=False, training=False):\n",
        "    x = inputs\n",
        "    x = self.embedding(x, training=training)\n",
        "    if states is None:\n",
        "      states = self.gru.get_initial_state(x)\n",
        "    x, states = self.gru(x, initial_state=states, training=training)\n",
        "    x = self.dense(x, training=training)\n",
        "\n",
        "    if return_state:\n",
        "      return x, states\n",
        "    else:\n",
        "      return x"
      ],
      "metadata": {
        "id": "UMmUN18l-gcw"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "model = MyModel(\n",
        "    vocab_size=vocab_size,\n",
        "    embedding_dim=embedding_dim,\n",
        "    rnn_units=rnn_units)"
      ],
      "metadata": {
        "id": "svzuLHAD-jRO",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 250
        },
        "outputId": "2b5596f0-ee55-4f46-9814-4ea4ae17d038"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'MyModel' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-5-92c1a4d604b5>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# @title\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m model = MyModel(\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mvocab_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0membedding_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0membedding_dim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     rnn_units=rnn_units)\n",
            "\u001b[0;31mNameError\u001b[0m: name 'MyModel' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Try Model"
      ],
      "metadata": {
        "id": "Ojp-n4VMS_eq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "    example_batch_predictions = model(input_example_batch)\n",
        "    print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S-DwQPzDTCg9",
        "outputId": "3f0effdc-8aed-4464-9648-1ac15dee0554"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(64, 100, 66) # (batch_size, sequence_length, vocab_size)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LkyT5roYZYNX",
        "outputId": "d044da65-ae5b-493a-88a8-e046c14cc9e0"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"my_model\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       multiple                  16896     \n",
            "                                                                 \n",
            " gru (GRU)                   multiple                  3938304   \n",
            "                                                                 \n",
            " dense (Dense)               multiple                  67650     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 4022850 (15.35 MB)\n",
            "Trainable params: 4022850 (15.35 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices, axis=-1).numpy()"
      ],
      "metadata": {
        "id": "kjIme1hoZbW9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "sampled_indices"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KqbzryEyZgRN",
        "outputId": "27a416dc-417d-47ec-90d6-606f9cc28b0a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([60,  6, 33, 30, 45, 61, 14, 27, 29, 14, 23, 63, 28, 48, 31, 22, 19,\n",
              "       14, 18, 27, 14, 64, 35, 44, 64, 50,  1, 17, 32, 47, 49, 34, 19, 54,\n",
              "       18, 21, 19, 15,  8, 25, 27, 42, 52, 18, 51, 36, 47,  4, 36, 56, 29,\n",
              "       57, 61, 15, 35, 64, 21, 48, 16, 40, 59,  1, 55, 15, 61, 17, 46, 63,\n",
              "       57, 30, 62, 15, 15,  7,  2, 11, 42, 14, 26, 41, 54,  9, 28, 52, 40,\n",
              "       44, 55, 44, 53, 48, 18, 58, 43,  1, 35, 49,  0, 29,  0, 22])"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "print(\"Input:\\n\", text_from_ids(input_example_batch[0]).numpy())\n",
        "print()\n",
        "print(\"Next Char Predictions:\\n\", text_from_ids(sampled_indices).numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eHFk0En8Zi2U",
        "outputId": "78a8339c-d632-4f1f-8a56-69cada994b65"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input:\n",
            " b' much:\\nLord, Lord, she will be a joyful woman.\\n\\nROMEO:\\nWhat wilt thou tell her, nurse? thou dost not'\n",
            "\n",
            "Next Char Predictions:\n",
            " b\"u'TQfvANPAJxOiRIFAENAyVeyk\\nDShjUFoEHFB-LNcmElWh$WqPrvBVyHiCat\\npBvDgxrQwBB, :cAMbo.OmaepeniEsd\\nVj[UNK]P[UNK]I\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the model"
      ],
      "metadata": {
        "id": "zCtYUkG8ZnS-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "loss = tf.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "lD7xRJ7pZoup"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "example_batch_mean_loss = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"Mean loss:        \", example_batch_mean_loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DGT5GHe8ZrE9",
        "outputId": "13e7d217-ce28-4238-9e3f-0ad0019bb7b5"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction shape:  (64, 100, 66)  # (batch_size, sequence_length, vocab_size)\n",
            "Mean loss:         tf.Tensor(4.190758, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "tf.exp(example_batch_mean_loss).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sz6sIxK2Zu4a",
        "outputId": "8418c126-c866-4056-a024-d08f90278a6b"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "66.07287"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "metadata": {
        "id": "tR06YVnJaCiQ"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "metadata": {
        "id": "Of3APZfZZ1ta"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "EPOCHS = 30"
      ],
      "metadata": {
        "id": "vTIEK8IeZ25u"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HdCXs8kgZ82u",
        "outputId": "9333b2b9-cb70-4167-bddc-8e95c4942ddd"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n",
            "172/172 [==============================] - 737s 4s/step - loss: 2.7128\n",
            "Epoch 2/30\n",
            "172/172 [==============================] - 729s 4s/step - loss: 1.9798\n",
            "Epoch 3/30\n",
            "172/172 [==============================] - 727s 4s/step - loss: 1.6986\n",
            "Epoch 4/30\n",
            "172/172 [==============================] - 726s 4s/step - loss: 1.5410\n",
            "Epoch 5/30\n",
            "172/172 [==============================] - 727s 4s/step - loss: 1.4452\n",
            "Epoch 6/30\n",
            "172/172 [==============================] - 728s 4s/step - loss: 1.3780\n",
            "Epoch 7/30\n",
            "172/172 [==============================] - 727s 4s/step - loss: 1.3260\n",
            "Epoch 8/30\n",
            "172/172 [==============================] - 728s 4s/step - loss: 1.2815\n",
            "Epoch 9/30\n",
            "172/172 [==============================] - 728s 4s/step - loss: 1.2405\n",
            "Epoch 10/30\n",
            "172/172 [==============================] - 727s 4s/step - loss: 1.2007\n",
            "Epoch 11/30\n",
            "172/172 [==============================] - 727s 4s/step - loss: 1.1611\n",
            "Epoch 12/30\n",
            "172/172 [==============================] - 728s 4s/step - loss: 1.1188\n",
            "Epoch 13/30\n",
            "172/172 [==============================] - 729s 4s/step - loss: 1.0755\n",
            "Epoch 14/30\n",
            "172/172 [==============================] - 728s 4s/step - loss: 1.0301\n",
            "Epoch 15/30\n",
            "172/172 [==============================] - 728s 4s/step - loss: 0.9809\n",
            "Epoch 16/30\n",
            "172/172 [==============================] - 730s 4s/step - loss: 0.9293\n",
            "Epoch 17/30\n",
            "172/172 [==============================] - 728s 4s/step - loss: 0.8759\n",
            "Epoch 18/30\n",
            "172/172 [==============================] - 729s 4s/step - loss: 0.8247\n",
            "Epoch 19/30\n",
            "172/172 [==============================] - 731s 4s/step - loss: 0.7736\n",
            "Epoch 20/30\n",
            "172/172 [==============================] - 730s 4s/step - loss: 0.7246\n",
            "Epoch 21/30\n",
            "172/172 [==============================] - 729s 4s/step - loss: 0.6809\n",
            "Epoch 22/30\n",
            "172/172 [==============================] - 727s 4s/step - loss: 0.6440\n",
            "Epoch 23/30\n",
            "172/172 [==============================] - 728s 4s/step - loss: 0.6097\n",
            "Epoch 24/30\n",
            "172/172 [==============================] - 726s 4s/step - loss: 0.5798\n",
            "Epoch 25/30\n",
            "172/172 [==============================] - 728s 4s/step - loss: 0.5538\n",
            "Epoch 26/30\n",
            "172/172 [==============================] - 727s 4s/step - loss: 0.5315\n",
            "Epoch 27/30\n",
            "172/172 [==============================] - 724s 4s/step - loss: 0.5151\n",
            "Epoch 28/30\n",
            "172/172 [==============================] - 722s 4s/step - loss: 0.5014\n",
            "Epoch 29/30\n",
            "172/172 [==============================] - 722s 4s/step - loss: 0.4872\n",
            "Epoch 30/30\n",
            "172/172 [==============================] - 720s 4s/step - loss: 0.4750\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "class OneStep(tf.keras.Model):\n",
        "  def __init__(self, model, chars_from_ids, ids_from_chars, temperature=1.0):\n",
        "    super().__init__()\n",
        "    self.temperature = temperature\n",
        "    self.model = model\n",
        "    self.chars_from_ids = chars_from_ids\n",
        "    self.ids_from_chars = ids_from_chars\n",
        "\n",
        "    # Create a mask to prevent \"[UNK]\" from being generated.\n",
        "    skip_ids = self.ids_from_chars(['[UNK]'])[:, None]\n",
        "    sparse_mask = tf.SparseTensor(\n",
        "        # Put a -inf at each bad index.\n",
        "        values=[-float('inf')]*len(skip_ids),\n",
        "        indices=skip_ids,\n",
        "        # Match the shape to the vocabulary\n",
        "        dense_shape=[len(ids_from_chars.get_vocabulary())])\n",
        "    self.prediction_mask = tf.sparse.to_dense(sparse_mask)\n",
        "\n",
        "  @tf.function\n",
        "  def generate_one_step(self, inputs, states=None):\n",
        "    # Convert strings to token IDs.\n",
        "    input_chars = tf.strings.unicode_split(inputs, 'UTF-8')\n",
        "    input_ids = self.ids_from_chars(input_chars).to_tensor()\n",
        "\n",
        "    # Run the model.\n",
        "    # predicted_logits.shape is [batch, char, next_char_logits]\n",
        "    predicted_logits, states = self.model(inputs=input_ids, states=states,\n",
        "                                          return_state=True)\n",
        "    # Only use the last prediction.\n",
        "    predicted_logits = predicted_logits[:, -1, :]\n",
        "    predicted_logits = predicted_logits/self.temperature\n",
        "    # Apply the prediction mask: prevent \"[UNK]\" from being generated.\n",
        "    predicted_logits = predicted_logits + self.prediction_mask\n",
        "\n",
        "    # Sample the output logits to generate token IDs.\n",
        "    predicted_ids = tf.random.categorical(predicted_logits, num_samples=1)\n",
        "    predicted_ids = tf.squeeze(predicted_ids, axis=-1)\n",
        "\n",
        "    # Convert from token ids to characters\n",
        "    predicted_chars = self.chars_from_ids(predicted_ids)\n",
        "\n",
        "    # Return the characters and model state.\n",
        "    return predicted_chars, states"
      ],
      "metadata": {
        "id": "3YaLcvyqatPr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "one_step_model = OneStep(model, chars_from_ids, ids_from_chars)"
      ],
      "metadata": {
        "id": "NDQVVNAmaxHF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "w8LJaExLa0p7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f0b60cd-f238-4337-8551-3bb8f1e4dea4"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Then she is in the matter of our hind perforking\n",
            "Or soldiers shall not might heaven presure to\n",
            "supper; but the kind senation as I pardon\n",
            "Forch their way, and me again the second nor clamosome\n",
            "Where she--mageted to rejoice and stones;\n",
            "And then, so often hast thou, whose person whate'er had been,\n",
            "For I love Juliet, a rather both of birch,\n",
            "The faults of honour, but you wrong'd,\n",
            "That in my choes world's health to the marnel,\n",
            "Nor how its country's stranger, being private messenger:\n",
            "Confined by a credot air, to hear you tell me who\n",
            "Hoverancies Kate, be bones to the appetite,\n",
            "To fetch a bad abroad curted,\n",
            "When I am unking'd brave belona,\n",
            "A man ado to tear upon our farmic each,\n",
            "That it great persuage it.\n",
            "\n",
            "First Watchman:\n",
            "Under thy mind of what is poisonous\n",
            "With a bowl that thou hast broke off freesy\n",
            "Thou lovest as trues,\n",
            "Woo Against the case! a brother of imprison me?\n",
            "\n",
            "HASTINGS:\n",
            "Only, good father, for they are almost friar,\n",
            "And consequence it out flaits.\n",
            "\n",
            "GLOUCESTER:\n",
            "The sun is wounded, since \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.5709919929504395\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result[0].numpy().decode('utf-8'), '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "Sd-ncuQUa23E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43161fed-198d-48bb-d727-2b19e4d42829"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "Whither?\n",
            "\n",
            "DORCAS:\n",
            "I would I keep it: Hearing God warlike prayers!\n",
            "They are free men, but I cannot get a meeting wit.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Marry, my lord, I claim you still. Pence me\n",
            "She lets that way, and mine art touchest to him:\n",
            "Yea, since it is some slander of the senate-shines,\n",
            "And many an honest malice oft;\n",
            "And that's worthy men! I have trusted him to\n",
            "the beggar-tate is your followers.\n",
            "\n",
            "First Lord:\n",
            "The heavens ghos, the which we have been\n",
            "Deteit upon him, he is choise unto the crown,\n",
            "That pain thy gubbers at all presumption.\n",
            "\n",
            "KING RICHARD III:\n",
            "Being bo, good sount!\n",
            "\n",
            "RICHOPFO:\n",
            "Sir William Lord Hastings and Menner\n",
            "since: you violaty, the all uncles begun,\n",
            "And thou a king: plain your queen and Learn'd each one fie!\n",
            "\n",
            "Provost:\n",
            "Who calls? keep you himself, for he hath something\n",
            "deared to hear? Who's he is, he would\n",
            "son--Gok, my lord, Edward for a king!\n",
            "Come, thou unpossession that condunt your wave\n",
            "He title in thy choice: then, farewell\n",
            "Of hy; thou'rt ill beloved, he\n",
            "Stands that an auguments o \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 1.5791990756988525\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "start = time.time()\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:', 'ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(1000):\n",
        "  next_char, states = one_step_model.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "result = tf.strings.join(result)\n",
        "end = time.time()\n",
        "print(result, '\\n\\n' + '_'*80)\n",
        "print('\\nRun time:', end - start)"
      ],
      "metadata": {
        "id": "f5Y0uCKza59g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f0a2da-5eee-4d3e-c46d-d59c6df223c7"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b\"ROMEO:\\nThe lands red me in the marriage of\\nWhich was out to pierce in a foe,\\nThough close with height more pitience and deep excales\\nTo fortune she is not in England's house;\\nHe shall help Clappis Beaugh'd a carencalation and\\nBy kingly sweet to be in fashionable as any\\ngirth, yet being a king, all eyes.\\n\\nFirst Citizen:\\nThen have you not add more, adieu!\\n\\nAUTOLYCUS:\\nI understand the business! where is he loves,\\nTo blot out mine, if the wager thou the beggary's bow:\\nSlew whos, say I am to Marcius come all,\\nTo give them thine out of change; and let him come hither:\\nThe goddess or hold cloud Richmond,\\nWhen you have not predect on nature stood,\\nAnd speign may deep duly downree again.\\n\\nROMEO:\\nThen, for my sake good speed your head, and be it so;\\nInswer'd thus, for my mage in my deeds,\\nTo brand my honour'd friendship in them,\\nThat do coil him not defend thine own defress,\\nA most worthy gentle queen in wrong.\\nBut that a hundred men call upon you\\nTo part the controating. Thrives, well-minger'd;\\nThat \"\n",
            " b\"ROMEO:\\nThere was a feeler thanky state, who was much,\\nAnd Rellain'd talk; in thou wast broken form,\\nHer pasant; you may have weep, so cures sun\\nWith all the swallows and the watch served\\nTo parden him that many feasts in 'suge, I\\nAre come in great duty with a fair correct,\\nAs this the sin unmoluhing lying tondy\\nThat's surful, that was made you dim?\\n\\nESCALIA:\\nWhat is your will have more to command the chase?\\nIs this business presunglant? for all things\\nAnd prightood to my wife, to rate him unjustly.\\n\\nQUEEN ELIZABETH:\\nA greater gift! O, purpose my lawful cett,\\nBecause she cries about a pride for her blush;\\nForthwith that cause to me well, I see your enemy's groans;\\nYet effections do put in my own, inform,\\nSought for him, draw not, that now in reedinish,\\nOr let both brother, the voices beniving\\nWere weakness, peace, cannot be very\\nTo ussured our blood to blank it all than smile.\\n\\nRICHMOND:\\nYes, yet it was in the use of them at Bristop her:\\nI have some meaner man or wounder, herein:\\nI never bear\"\n",
            " b\"ROMEO:\\nThis is a beggarly and unrest, and children\\nWith open our subjects for what his years\\nAre made against the lives of knightly send;\\nThe present war thou turn a connurture,\\nThat seem'd a prize of receitn. To the souls.\\n\\nKATHARINA:\\nNow, first consul: you mistake me much.\\n\\nGLOUCESTER:\\nSo did you so steward? Must I go forward\\nshall go about highness. \\nDERBY:\\nSoft to thy bases! Mannifal you at eind heads the\\nprince, that can we quit with a fresh presages,\\nIf any blood your heart to sink,\\nI'll not to be so redessibled.\\n\\nESCALUS:\\nAy.\\n\\nPETRUCHIO:\\nRower Speak soul thou Lord Hastings, who attended hour?\\n\\nABROHAO:\\nHis son your suet--\\nWhich we have bled togethern to commit me\\nTo furrow and tear the sun seems gree.\\n\\nSIRINAUDETS:\\nHercelding of that choice is not disgraced, but they\\naccept me, and is it lose the famous of him,\\nThe stringes drunk upon him. Come, I'll die thy\\npartner fire by journey.\\n\\nROMEO:\\nI hear his head, quickly say, what's outin'er\\nAs I would your frown fair ladies.\\n\\nGRUMIO:\\nA pro\"\n",
            " b\"ROMEO:\\nThese eyes cannot bring yield to him and the business\\nOf their awfilour company.\\n\\nVOLUMNIA:\\n'Tis well that I have done too much:\\nA promiser ty, so fly to 't:\\nhorn't usurp; the sweet birisle,\\nIn this our fair admiral: I have learn'd with noble\\nBut who should say, sir? I will content you; I spread thy\\ntold;\\nYou will repost by good morrow here in blood,\\nEre he cannot choose but gentle heavier.\\n\\nKING RICHARD III:\\nI thank your grace.\\n\\nGLOUCESTER:\\nIf I do now, my lord.\\n\\nKING RICHARD III:\\nI would the way be convey'd.\\n\\nANTONIO:\\nDraw: turn back, dress'd him up with him.\\n\\nBENVOLIO:\\nAt thy general.\\n\\nAUFIDIUS:\\nWhat confound a word with you.\\n\\nABROSOO:\\nHe is invereign, but not to them accomplan;\\nAnd fetch him careful war in power did\\nThe crown, you were in all the band is old\\nA bird heart-sedarity.\\n\\nPETRUCHIO:\\nNow, for my life, the gods beginn more rages of honour,\\nMust gently one hour in their infasymang\\nAs 'twere a braiseth wrong us art and grief:\\nAnd his bellow'd and not of blood, descend\\nAnd ta\"\n",
            " b\"ROMEO:\\nThere is no wretched world, indeed!\\nThe man I am, good fortune to the ground.\\nYou speak a worm of this: I straight from me this,\\nYou bounted, in a poor parents, for some sound\\nWith some unwarpation of thy state and parchment of time\\nrenefally he is always groan and sadls him\\nTo the shume would curse that runes the wisest cherbs\\nfor him will do't content you more than any?\\n\\nTRANIO:\\nAt thy good hap.\\nCome upon you! Tell my life and goodfully.\\n\\nGREMIO:\\nAnd when the dark night can make Warwicks.\\n\\nKING HENRY VI:\\nFarewell, all virtue, and expect him to\\nthe base delicious humour.\\nBut can no Friar, Warwick, Edward's ground,\\nThe letter was not in blood to power these fair restitute,\\nLone eyes must blood at home, my soul,\\nAnd now I well and I from this controve more\\nIn honour after,ir in her hand.\\nSound too much mis-shaped till not one break of out,\\nAnd still 'tis time.\\n\\nLEONTES:\\nTo your compary, small ask again.\\nI will provoke your grace: how sweet hast bottled soul!\\nThy flattering swords of bl\"], shape=(5,), dtype=string) \n",
            "\n",
            "________________________________________________________________________________\n",
            "\n",
            "Run time: 2.9562084674835205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "tf.saved_model.save(one_step_model, 'one_step')\n",
        "one_step_reloaded = tf.saved_model.load('one_step')"
      ],
      "metadata": {
        "id": "HbRoTSBWa83e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7eb7d85d-2a55-455b-98c5-bf374b8110b2"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Skipping full serialization of Keras layer <__main__.OneStep object at 0x78da42734790>, because it is not built.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n",
            "WARNING:tensorflow:Model's `__init__()` arguments contain non-serializable objects. Please implement a `get_config()` method in the subclassed Model for proper saving and loading. Defaulting to empty config.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "states = None\n",
        "next_char = tf.constant(['ROMEO:'])\n",
        "result = [next_char]\n",
        "\n",
        "for n in range(100):\n",
        "  next_char, states = one_step_reloaded.generate_one_step(next_char, states=states)\n",
        "  result.append(next_char)\n",
        "\n",
        "print(tf.strings.join(result)[0].numpy().decode(\"utf-8\"))"
      ],
      "metadata": {
        "id": "9ZWiK8_8bAom",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "412dab4b-efb7-47af-8f10-781034b8256b"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROMEO:\n",
            "You have been in commody that tulls about,\n",
            "I was another button on:\n",
            "Let him be false, pounds your h\n"
          ]
        }
      ]
    }
  ]
}